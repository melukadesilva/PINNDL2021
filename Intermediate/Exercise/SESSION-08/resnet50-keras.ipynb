{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Paper https://arxiv.org/pdf/1512.03385.pdf","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 64","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cifar10 = tf.keras.datasets.cifar10\n(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\ntrain_images = train_images / 255.0\ntest_images = test_images / 255.0\n\nval_images = train_images[-10000:]\nval_labels = train_labels[-10000:]\ntrain_images = train_images[:-10000]\ntrain_labels = train_labels[:-10000]\n\nTRAIN_IMG_COUNT = train_labels.shape[0]\nVAL_IMG_COUNT = val_labels.shape[0]\nprint(train_labels.shape)\nprint(val_labels.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"AUTOTUNE = tf.data.experimental.AUTOTUNE ## Auto tune tf.data hyper parameters for \n\ndef prepare_for_training(ds, cache=False, shuffle_buffer_size=1000):\n    # This is a small dataset, only load it once, and keep it in memory.\n    # use `.cache(filename)` to cache preprocessing work for datasets that don't\n    # fit in memory.\n    if cache:\n        if isinstance(cache, str): ## if caching to a file\n            ds = ds.cache(cache)\n        else: ## cache in memory\n            ds = ds.cache()\n    ## shuffle data, otherwise, data will be cached with the same patten (not random)\n    ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n\n    # Repeat forever (Reinitialise the dataset after each training iteration)\n    ds = ds.repeat()\n\n    ds = ds.batch(BATCH_SIZE)\n\n    # `prefetch` lets the dataset fetch batches in the background while the model\n    # is training.\n    ds = ds.prefetch(buffer_size=AUTOTUNE)\n\n    return ds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_labels_onehot =  tf.keras.utils.to_categorical(train_labels, 10)\nval_labels_onehot = tf.keras.utils.to_categorical(val_labels, 10)\nprint(train_labels.shape)\ntrain_ds_tensor = tf.data.Dataset.from_tensor_slices((train_images, train_labels_onehot))\nval_ds_tensor = tf.data.Dataset.from_tensor_slices((val_images, val_labels_onehot))\n\ntrain_ds = prepare_for_training(train_ds_tensor)\nval_ds = prepare_for_training(val_ds_tensor)\n\nfor d in train_ds.take(1):\n    print(d[1].shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<img src=\"https://d2l.ai/_images/resnet-block.svg\"/>","metadata":{}},{"cell_type":"code","source":"# Residual block (Identity)\ndef identity_block(x, filters, kernel_size, kernel_regularizer, kernel_initializer):\n    x_id = x\n    filter1, filter2, filter3 = filters # unpack the list of filters\n    x = tf.keras.layers.Conv2D(\n                                filter1, (1,1), strides=(1, 1), padding='valid',\n                                data_format=None, dilation_rate=(1, 1), groups=1, activation=None,\n                                use_bias=True, kernel_initializer=kernel_initializer,\n                                bias_initializer='zeros', kernel_regularizer=kernel_regularizer,\n                                bias_regularizer=None, activity_regularizer=None, kernel_constraint=None,\n                                bias_constraint=None\n                            )(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.activations.relu(x)\n    \n    x = tf.keras.layers.Conv2D(\n                                filter2, (kernel_size, kernel_size), padding='same',\n                                kernel_initializer=kernel_initializer,\n                                kernel_regularizer=kernel_regularizer\n                            )(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.activations.relu(x)\n    \n    x = tf.keras.layers.Conv2D(\n                                filter3, (1, 1),\n                                kernel_initializer=kernel_initializer,\n                                kernel_regularizer=kernel_regularizer\n                            )(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    \n    x = tf.keras.layers.Add()([x, x_id])\n    x = tf.keras.activations.relu(x)\n    \n    return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Residual block (Identity)\ndef conv_block(x, filters, kernel_size, kernel_regularizer, kernel_initializer, strides=(2,2)):\n    filter1, filter2, filter3 = filters # unpack the list of filters\n    x_id = tf.keras.layers.Conv2D(\n                                filter3, (1,1), strides=strides,\n                                kernel_initializer=kernel_initializer,\n                                kernel_regularizer=kernel_regularizer,\n                            )(x)\n    \n\n    x = tf.keras.layers.Conv2D(\n                                filter1, (1,1), strides=strides,\n                                kernel_initializer=kernel_initializer,\n                                kernel_regularizer=kernel_regularizer,\n                            )(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.activations.relu(x)\n    \n    x = tf.keras.layers.Conv2D(\n                                filter2, (kernel_size, kernel_size), padding='same',\n                                kernel_initializer=kernel_initializer,\n                                kernel_regularizer=kernel_regularizer\n                            )(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.activations.relu(x)\n    \n    x = tf.keras.layers.Conv2D(\n                                filter3, (1, 1),\n                                kernel_initializer=kernel_initializer,\n                                kernel_regularizer=kernel_regularizer\n                            )(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    \n    x = tf.keras.layers.Add()([x, x_id])\n    x = tf.keras.activations.relu(x)\n    \n    return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function test of the blocks\ninp = tf.random.normal((1, 32, 32, 256))\nout = identity_block(inp, [64,64,256], 3, tf.keras.regularizers.L2(), tf.keras.initializers.HeNormal())\nprint(out.shape)\n\ninp = tf.random.normal((1, 32, 32, 3))\nout = conv_block(inp, [64,64,256], 3, tf.keras.regularizers.L2(), tf.keras.initializers.HeNormal())\nprint(out.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def resnet50(input_shape, \n             kernel_regularizer=tf.keras.regularizers.L2, \n             kernel_initializer=tf.keras.initializers.HeNormal,\n             classes=10):\n    # Input layer\n    inputs = tf.keras.layers.Input(input_shape)\n    '''\n    x = tf.keras.layers.Lambda( \n                        lambda image: tf.image.resize( \n                            image, \n                            (224, 224), \n                            method = tf.image.ResizeMethod.BICUBIC,\n                            preserve_aspect_ratio = True\n                            )\n                        )(inputs)\n    '''\n    # First level of feature extraction\n    x = tf.keras.layers.Conv2D(\n                                64, (7, 7), strides=(2, 2),\n                                kernel_initializer=kernel_initializer,\n                                kernel_regularizer=kernel_regularizer\n                            )(inputs)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.activations.relu(x)\n    x = tf.keras.layers.MaxPooling2D((3, 3), strides=(2, 2))(x)\n    \n    # Resnet blocks\n    x = conv_block(x, [64, 64, 256], 3, kernel_regularizer, kernel_initializer, strides=(1, 1))\n    x = identity_block(x, [64, 64, 256], 3, kernel_regularizer, kernel_initializer)\n    x = identity_block(x, [64, 64, 256], 3, kernel_regularizer, kernel_initializer)\n\n    x = conv_block(x, [128, 128, 512], 3, kernel_regularizer, kernel_initializer)\n    x = identity_block(x, [128, 128, 512], 3, kernel_regularizer, kernel_initializer)\n    x = identity_block(x, [128, 128, 512], 3, kernel_regularizer, kernel_initializer)\n    x = identity_block(x, [128, 128, 512], 3, kernel_regularizer, kernel_initializer)\n\n    x = conv_block(x, [256, 256, 1024], 3, kernel_regularizer, kernel_initializer)\n    x = identity_block(x, [256, 256, 1024], 3, kernel_regularizer, kernel_initializer)\n    x = identity_block(x, [256, 256, 1024], 3, kernel_regularizer, kernel_initializer)\n    x = identity_block(x, [256, 256, 1024], 3, kernel_regularizer, kernel_initializer)\n    x = identity_block(x, [256, 256, 1024], 3, kernel_regularizer, kernel_initializer)\n    x = identity_block(x, [256, 256, 1024], 3, kernel_regularizer, kernel_initializer)\n\n    x = conv_block(x, [512, 512, 2048], 3, kernel_regularizer, kernel_initializer)\n    x = identity_block(x, [512, 512, 2048], 3, kernel_regularizer, kernel_initializer)\n    x = identity_block(x, [512, 512, 2048], 3, kernel_regularizer, kernel_initializer)\n        \n    # x = tf.keras.layers.AveragePooling2D((7, 7))(x)\n    \n    x = tf.keras.layers.Flatten()(x)\n    \n    x = tf.keras.layers.Dense(classes, activation=\"softmax\")(x)\n\n    model = tf.keras.Model(inputs, x)\n    \n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inp = tf.random.normal((1, 32, 32, 3))\nmodel = resnet50((32, 32, 3), tf.keras.regularizers.L1(1e-2), tf.keras.initializers.HeNormal())\nout = model(inp)\nprint(out.shape)\nprint(model.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"METRICS = [\n    tf.keras.metrics.CategoricalAccuracy(),\n    tf.keras.metrics.Precision(name='precision'),\n    tf.keras.metrics.Recall(name='recall')\n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"opt = tf.keras.optimizers.SGD(learning_rate=1e-3)\n\n# Prepare the model for training\nmodel.compile(optimizer=opt, # Stochastic gradient descent optimiser\n              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False), # Crossentropy loss\n              metrics=METRICS\n             ) # Accuracy measure","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## To prevent the model from becoming worst (e.g. overfitting) stop the training before the issues start using the Early stopping callback\nearly_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10,\n                                                     restore_best_weights=True)\n\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs\")\n\nhistory = model.fit(\n    train_ds,\n    steps_per_epoch=TRAIN_IMG_COUNT // BATCH_SIZE,\n    epochs=100,\n    validation_data=val_ds,\n    validation_steps=VAL_IMG_COUNT // BATCH_SIZE,\n    # callbacks=[early_stopping_cb]\n    callbacks=[tensorboard_callback]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n## visualize the performance\nfig, ax = plt.subplots(1, 4, figsize=(20, 3))\nax = ax.ravel()\nprint(history.history.keys())\n\nfor i, met in enumerate(['precision', 'recall', 'categorical_accuracy', 'loss']):\n    ax[i].plot(history.history[met])\n    ax[i].plot(history.history['val_' + met])\n    ax[i].set_title('Model {}'.format(met))\n    ax[i].set_xlabel('epochs')\n    ax[i].set_ylabel(met)\n    ax[i].legend(['train', 'val'])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}