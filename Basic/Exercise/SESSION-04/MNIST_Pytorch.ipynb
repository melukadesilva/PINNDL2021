{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_Pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-JJLI0aFtS-"
      },
      "source": [
        "# Pytorch MNIST example\n",
        "#### https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
        "#### https://torchmetrics.readthedocs.io/en/latest/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OejQlE_f7oGB",
        "outputId": "9ecf77c1-cce4-4623-eecd-7bdfaaa1e634"
      },
      "source": [
        "!pip install torchmetrics"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.7/dist-packages (0.5.1)\n",
            "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.9.0+cu111)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.1->torchmetrics) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (2.4.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qzl0MKcCcqH6"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchmetrics\n",
        "import torchvision"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lF8PnirVct06"
      },
      "source": [
        "transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "dataset1 = torchvision.datasets.MNIST('../data', train=True, transform=transform, download=True)\n",
        "dataset2 = torchvision.datasets.MNIST('../data', train=False, transform=transform)"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWTEro5idpFs",
        "outputId": "0c502410-80dd-4f14-950e-3c6c3f896c36"
      },
      "source": [
        "print(dataset1)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset MNIST\n",
            "    Number of datapoints: 60000\n",
            "    Root location: ../data\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               ToTensor()\n",
            "               Normalize(mean=(0.1307,), std=(0.3081,))\n",
            "           )\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMk4gAUod4sZ"
      },
      "source": [
        "# Load the mnist data\n",
        "train_kwargs = {'batch_size': 32}\n",
        "test_kwargs = {'batch_size': 10}\n",
        "train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adTXCNmJ2X8c"
      },
      "source": [
        "# Model\n",
        "model = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(784, 40),\n",
        "    nn.Linear(40, 30),\n",
        "    nn.Linear(30, 20),\n",
        "    nn.Linear(20, 10),\n",
        ")"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yZtJtNr3vdo",
        "outputId": "51372db9-a11e-464a-f3d6-cefef2c26300"
      },
      "source": [
        "print(model)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): Flatten(start_dim=1, end_dim=-1)\n",
            "  (1): Linear(in_features=784, out_features=40, bias=True)\n",
            "  (2): Linear(in_features=40, out_features=30, bias=True)\n",
            "  (3): Linear(in_features=30, out_features=20, bias=True)\n",
            "  (4): Linear(in_features=20, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W123D1OZ3xXW"
      },
      "source": [
        "# Set loss function and the optimiser\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), 0.001)"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDCVzunb4fZE"
      },
      "source": [
        "def train(metric_train):\n",
        "  model.train()\n",
        "  for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "    # set the gradients to zero (flush accumilated gradients)\n",
        "    optimizer.zero_grad()\n",
        "    # Do a prediction step on a batch of images\n",
        "    predictions = model(images)\n",
        "    # Calculate the loss\n",
        "    loss = loss_fn(predictions, labels)\n",
        "    # Do a backward step to compute the model gradients\n",
        "    loss.backward()\n",
        "    # Update the parameters of the model\n",
        "    optimizer.step()\n",
        "\n",
        "    metric_train(predictions, labels)\n",
        "  acc = metric_train.compute()\n",
        "  \n",
        "  return acc"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtYSqzdW_sF8"
      },
      "source": [
        "# Validation step\n",
        "def validate(metric_val):\n",
        "  model.eval() # put the model into evaluation mode\n",
        "  # Take the model without the gradients\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, (images, labels) in enumerate(test_loader):\n",
        "      # do a prediction\n",
        "      predictions = model(images)\n",
        "      # compute accuracy\n",
        "      metric_val(predictions, labels)\n",
        "\n",
        "  acc = metric_val.compute()\n",
        "  return acc"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjShX9fK79cU",
        "outputId": "743f50de-fc08-49c8-be3e-05ac26ae9572"
      },
      "source": [
        "# initialize metric\n",
        "metric_train = torchmetrics.Accuracy()\n",
        "metric_val = torchmetrics.Accuracy()\n",
        "# train the model\n",
        "epochs = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  train_acc = train(metric_train)\n",
        "  print(f\"Train Accuracy on epoch {epoch}: {train_acc}\")\n",
        "  val_acc = validate(metric_val)\n",
        "  print(f\"Validation Accuracy on epoch {epoch}: {val_acc}\")"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy on epoch 0: 0.48401665687561035\n",
            "Validation Accuracy on epoch 0: 0.6793000102043152\n",
            "Train Accuracy on epoch 1: 0.6211666464805603\n",
            "Validation Accuracy on epoch 1: 0.7458500266075134\n",
            "Train Accuracy on epoch 2: 0.6917222142219543\n",
            "Validation Accuracy on epoch 2: 0.7847333550453186\n",
            "Train Accuracy on epoch 3: 0.7354999780654907\n",
            "Validation Accuracy on epoch 3: 0.8092749714851379\n",
            "Train Accuracy on epoch 4: 0.7650666832923889\n",
            "Validation Accuracy on epoch 4: 0.8258799910545349\n",
            "Train Accuracy on epoch 5: 0.7862389087677002\n",
            "Validation Accuracy on epoch 5: 0.8378000259399414\n",
            "Train Accuracy on epoch 6: 0.8021023869514465\n",
            "Validation Accuracy on epoch 6: 0.8467857241630554\n",
            "Train Accuracy on epoch 7: 0.8145020604133606\n",
            "Validation Accuracy on epoch 7: 0.8539000153541565\n",
            "Train Accuracy on epoch 8: 0.8244647979736328\n",
            "Validation Accuracy on epoch 8: 0.8598111271858215\n",
            "Train Accuracy on epoch 9: 0.8326166868209839\n",
            "Validation Accuracy on epoch 9: 0.8647199869155884\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUnRvPaWA0SS"
      },
      "source": [
        ""
      ],
      "execution_count": 89,
      "outputs": []
    }
  ]
}