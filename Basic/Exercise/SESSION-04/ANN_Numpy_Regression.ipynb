{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RizJ87Koqmyo"
      },
      "outputs": [],
      "source": [
        "#### Implementation of ANN with SGD and Backprop\n",
        "#### original OOP implementation can be found at: https://medium.com/binaryandmore/beginners-guide-to-deriving-and-implementing-backpropagation-e3c1a5a1e536\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "q4EDmtpGqrSp"
      },
      "outputs": [],
      "source": [
        "## layer:\n",
        "## take input X: (Nx1)\n",
        "## multiply by weights W: (h1xN)\n",
        "## add bias b: (h1x1)\n",
        "def layer(X, W, b): return np.dot(W, X) + b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HJuGPvyeq16g"
      },
      "outputs": [],
      "source": [
        "## lets take the activation as sigmoid (element-wise)\n",
        "def sigmoid(x): return 1 / (1 + np.exp(-x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9imKk4JYq4GK"
      },
      "outputs": [],
      "source": [
        "## lets define the derivative of the sigmoid\n",
        "def sigmoid_prime(x): return sigmoid(x)*(1.0 - sigmoid(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iaq1pJQZrAC8"
      },
      "outputs": [],
      "source": [
        "## define the mse cost per sample\n",
        "def mse(y_pred, y_true): return (1 / 2) * np.square(y_pred - y_true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tPAw295HrBzQ"
      },
      "outputs": [],
      "source": [
        "## derivative of mse\n",
        "def mse_prime(y_pred, y_true): return (y_pred - y_true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FDcljZLGrD_5"
      },
      "outputs": [],
      "source": [
        "## NN parameters\n",
        "N = 50 #input size\n",
        "num_nes = [N, 40, 30, 20, 10]\n",
        "num_layers = len(num_nes) - 1\n",
        "learning_rate = 0.01\n",
        "num_epochs = 1000\n",
        "num_data_samples = 100\n",
        "\n",
        "## dict to hold neuron input-output pairs\n",
        "layer_in_out_dict = dict()\n",
        "## dict to hold the cost derivatives w.r.t to the params\n",
        "layer_derivative_dict = dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Bb2a_irKriMT"
      },
      "outputs": [],
      "source": [
        "## the dict to hold the layer weights and biases\n",
        "weight_dict = {'w'+str(i): np.random.normal(size=(num_nes[i+1], num_nes[i])) for i in range(num_layers)}\n",
        "bias_dict = {'b'+str(i): np.random.normal(size=(num_nes[i+1], 1)) for i in range(num_layers)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Sz7Zl5pArjsl"
      },
      "outputs": [],
      "source": [
        "def NN(inputs):\n",
        "    a = inputs\n",
        "    layer_in_out_dict['a0'] = a\n",
        "    ## forward pass\n",
        "    for i in range(num_layers):\n",
        "        #print(i)\n",
        "        z = layer(a, weight_dict['w'+str(i)], bias_dict['b'+str(i)])\n",
        "        a = sigmoid(z)\n",
        "        layer_in_out_dict['z'+str(i)] = z\n",
        "        layer_in_out_dict['a'+str(i+1)] = a\n",
        "    out = a\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47HzaxV1rl5U",
        "outputId": "ec8d6f6e-1baf-492d-c5d6-20e3c2ec025a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.11920851217449469\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/ty/pxb5sqj175xg9rl8xpmdxlkc0000gn/T/ipykernel_37348/2668785541.py:2: RuntimeWarning: overflow encountered in exp\n",
            "  def sigmoid(x): return 1 / (1 + np.exp(-x))\n"
          ]
        }
      ],
      "source": [
        "## Test the NN with a dummy forward pass\n",
        "inp = np.random.uniform(0, 1, size=(N,1)) * 255.0\n",
        "out_true = np.random.uniform(0, 1, size=(num_nes[-1], 1))\n",
        "out = NN(inp)\n",
        "error = mse(out, out_true)\n",
        "print(np.mean(error))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "br6a79OyroAQ"
      },
      "outputs": [],
      "source": [
        "## Function to perform model prediction\n",
        "def predict(inputs): return NN(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "cICecJuwrqFr"
      },
      "outputs": [],
      "source": [
        "def compute_grads(inputs, targets):\n",
        "    ## perform a forward pass (prediction)\n",
        "    out = predict(inputs)\n",
        "    ## Compute dC/dzL, dC/dwL and dC/dbL\n",
        "    ## first dC/dzL = (a - y) * a * (1 - a)\n",
        "    layer_derivative_dict['dz'+str(num_layers-1)] = mse_prime(out, targets) * out * (1 - out)\n",
        "    ## second compute the dC/dwL = dC/dzL * dzL/dwL\n",
        "    #print(layer_derivative_dict['dz'+str(num_layers-1)].shape)\n",
        "    #print(layer_in_out_dict['a'+str(num_layers-1)].shape)\n",
        "    layer_derivative_dict['dw'+str(num_layers-1)] = np.dot(layer_derivative_dict['dz'+str(num_layers-1)], \\\n",
        "                                                    np.transpose(layer_in_out_dict['a'+str(num_layers-1)]))\n",
        "    ## third calculate the dC/dbL = dC/dzL * dzL/dbL\n",
        "    layer_derivative_dict['db'+str(num_layers-1)] = layer_derivative_dict['dz'+str(num_layers-1)] * 1\n",
        "    \n",
        "    ## Now since we have the derivatives at the output layer, we can calculate the derivatives for the rest of the \n",
        "    ## layers\n",
        "    for i in reversed(range(num_layers-1)):\n",
        "        #print(i)\n",
        "        ## first compute dC/dzl = [w(l+1)*dC/dz(l+1)]*sigmoid_prime(zl)\n",
        "        layer_derivative_dict['dz'+str(i)] = np.dot(\n",
        "                                                np.transpose(weight_dict['w'+str(i+1)]), layer_derivative_dict['dz'+str(i+1)]) \\\n",
        "                                                * sigmoid_prime(layer_in_out_dict['z'+str(i)])\n",
        "        ## second compute dC/dwl = dC/dzl * dzl/dwl\n",
        "        layer_derivative_dict['dw'+str(i)] = layer_derivative_dict['dz'+str(i)] * \\\n",
        "                                                    np.transpose(layer_in_out_dict['a'+str(i)])\n",
        "        \n",
        "        ## third compute dC/dbl = dC/dzl * dzl/dbl\n",
        "        layer_derivative_dict['db'+str(i)] = layer_derivative_dict['dz'+str(i)] * 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOuOcuTJrsVL",
        "outputId": "44b2034a-f7f0-42b3-bf66-19ce9f628fda"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/ty/pxb5sqj175xg9rl8xpmdxlkc0000gn/T/ipykernel_37348/2668785541.py:2: RuntimeWarning: overflow encountered in exp\n",
            "  def sigmoid(x): return 1 / (1 + np.exp(-x))\n"
          ]
        }
      ],
      "source": [
        "## test the gradient computation \n",
        "compute_grads(inp, out_true)\n",
        "#print(layer_derivative_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "z3wPaX7Yrt5R"
      },
      "outputs": [],
      "source": [
        "def update_param():\n",
        "    for i in range(num_layers):\n",
        "        weight_dict['w'+str(i)] = weight_dict['w'+str(i)] - learning_rate * layer_derivative_dict['dw'+str(i)]\n",
        "        bias_dict['b'+str(i)] = bias_dict['b'+str(i)] - learning_rate * layer_derivative_dict['db'+str(i)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "hgAm-tsZrwPO"
      },
      "outputs": [],
      "source": [
        "## test param update function\n",
        "update_param()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "r4ogIKRNrx8h"
      },
      "outputs": [],
      "source": [
        "## make a dummy training dataset\n",
        "inp_data = np.random.normal(size=(num_data_samples, num_nes[0]))\n",
        "out_true_data = np.random.normal(size=(num_data_samples, num_nes[-1]))\n",
        "#print(training_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DX3bR1Ngrzvg",
        "outputId": "6321f038-1545-47f4-836d-49c0be3d1097"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch loss: 0.5917002996873171\n",
            "Epoch loss: 0.5519143856719242\n",
            "Epoch loss: 0.5246789448715947\n",
            "Epoch loss: 0.5136013997693294\n",
            "Epoch loss: 0.5088261533541005\n",
            "Epoch loss: 0.5062083544199996\n",
            "Epoch loss: 0.5045177968886015\n",
            "Epoch loss: 0.5032897363412442\n",
            "Epoch loss: 0.5023161093918734\n",
            "Epoch loss: 0.5014927602504633\n",
            "Epoch loss: 0.5007630071249259\n",
            "Epoch loss: 0.5000939020715546\n",
            "Epoch loss: 0.49946525614803405\n",
            "Epoch loss: 0.4988641683518793\n",
            "Epoch loss: 0.4982821162358633\n",
            "Epoch loss: 0.49771331683757153\n",
            "Epoch loss: 0.4971537555830451\n",
            "Epoch loss: 0.4966005885798316\n",
            "Epoch loss: 0.4960517676774778\n",
            "Epoch loss: 0.4955058067081508\n",
            "Epoch loss: 0.4949616408040526\n",
            "Epoch loss: 0.4944185471011327\n",
            "Epoch loss: 0.49387610326642917\n",
            "Epoch loss: 0.4933341644216921\n",
            "Epoch loss: 0.4927928417970167\n",
            "Epoch loss: 0.492252470003221\n",
            "Epoch loss: 0.4917135556929674\n",
            "Epoch loss: 0.4911767085055528\n",
            "Epoch loss: 0.49064256332293893\n",
            "Epoch loss: 0.49011170762939593\n",
            "Epoch loss: 0.489584626941056\n",
            "Epoch loss: 0.48906167551550966\n",
            "Epoch loss: 0.48854307197251545\n",
            "Epoch loss: 0.48802891346395816\n",
            "Epoch loss: 0.48751919943538147\n",
            "Epoch loss: 0.4870138566652437\n",
            "Epoch loss: 0.48651275986077486\n",
            "Epoch loss: 0.48601574521669966\n",
            "Epoch loss: 0.4855226170214818\n",
            "Epoch loss: 0.4850331491595702\n",
            "Epoch loss: 0.4845470841290694\n",
            "Epoch loss: 0.4840641321255887\n",
            "Epoch loss: 0.48358397210470705\n",
            "Epoch loss: 0.48310625583198297\n",
            "Epoch loss: 0.4826306150325605\n",
            "Epoch loss: 0.4821566710495291\n",
            "Epoch loss: 0.48168404598942677\n",
            "Epoch loss: 0.4812123741549144\n",
            "Epoch loss: 0.48074131256235114\n",
            "Epoch loss: 0.48027054943584424\n",
            "Epoch loss: 0.4797998097183009\n",
            "Epoch loss: 0.4793288568569439\n",
            "Epoch loss: 0.47885749045303233\n",
            "Epoch loss: 0.47838553984804816\n",
            "Epoch loss: 0.477912854321336\n",
            "Epoch loss: 0.47743929117713363\n",
            "Epoch loss: 0.4769647034217362\n",
            "Epoch loss: 0.4764889288143133\n",
            "Epoch loss: 0.4760117817674201\n",
            "Epoch loss: 0.47553304897319465\n",
            "Epoch loss: 0.47505248894175756\n",
            "Epoch loss: 0.4745698350711834\n",
            "Epoch loss: 0.47408480155427263\n",
            "Epoch loss: 0.4735970913811791\n",
            "Epoch loss: 0.473106405842242\n",
            "Epoch loss: 0.47261245515632694\n",
            "Epoch loss: 0.4721149700371665\n",
            "Epoch loss: 0.4716137140833897\n",
            "Epoch loss: 0.47110849679042077\n",
            "Epoch loss: 0.47059918672004747\n",
            "Epoch loss: 0.47008572395037823\n",
            "Epoch loss: 0.4695681304429942\n",
            "Epoch loss: 0.4690465165520875\n",
            "Epoch loss: 0.4685210817702244\n",
            "Epoch loss: 0.4679921081703409\n",
            "Epoch loss: 0.4674599459722617\n",
            "Epoch loss: 0.466924992116876\n",
            "Epoch loss: 0.46638766428581446\n",
            "Epoch loss: 0.4658483739197634\n",
            "Epoch loss: 0.4653075020254762\n",
            "Epoch loss: 0.46476538082252317\n",
            "Epoch loss: 0.46422228286432654\n",
            "Epoch loss: 0.4636784176948047\n",
            "Epoch loss: 0.4631339348445865\n",
            "Epoch loss: 0.46258893126910405\n",
            "Epoch loss: 0.4620434611748609\n",
            "Epoch loss: 0.4614975464103734\n",
            "Epoch loss: 0.4609511860253735\n",
            "Epoch loss: 0.4604043640808222\n",
            "Epoch loss: 0.45985705524131804\n",
            "Epoch loss: 0.45930922806666225\n",
            "Epoch loss: 0.45876084622886687\n",
            "Epoch loss: 0.4582118681054171\n",
            "Epoch loss: 0.457662245326376\n",
            "Epoch loss: 0.45711192087269675\n",
            "Epoch loss: 0.4565608272427646\n",
            "Epoch loss: 0.4560088850538587\n",
            "Epoch loss: 0.4554560022731303\n",
            "Epoch loss: 0.45490207412908956\n",
            "Epoch loss: 0.45434698367429294\n",
            "Epoch loss: 0.4537906029631554\n",
            "Epoch loss: 0.4532327948638729\n",
            "Epoch loss: 0.4526734156173136\n",
            "Epoch loss: 0.4521123183651858\n",
            "Epoch loss: 0.4515493579773507\n",
            "Epoch loss: 0.45098439760053266\n",
            "Epoch loss: 0.45041731741015967\n",
            "Epoch loss: 0.44984802603769475\n",
            "Epoch loss: 0.44927647499831624\n",
            "Epoch loss: 0.44870267604812947\n",
            "Epoch loss: 0.44812672062413594\n",
            "Epoch loss: 0.44754879928221186\n",
            "Epoch loss: 0.44696921746174667\n",
            "Epoch loss: 0.446388402456104\n",
            "Epoch loss: 0.44580689606534113\n",
            "Epoch loss: 0.44522532906417867\n",
            "Epoch loss: 0.44464437767213333\n",
            "Epoch loss: 0.4440647075479772\n",
            "Epoch loss: 0.4434869150122705\n",
            "Epoch loss: 0.44291147594166325\n",
            "Epoch loss: 0.4423387096953681\n",
            "Epoch loss: 0.4417687603737593\n",
            "Epoch loss: 0.44120159325752967\n",
            "Epoch loss: 0.44063700197805233\n",
            "Epoch loss: 0.44007462179312135\n",
            "Epoch loss: 0.43951394537274724\n",
            "Epoch loss: 0.4389543388545605\n",
            "Epoch loss: 0.4383950571995914\n",
            "Epoch loss: 0.4378352591367134\n",
            "Epoch loss: 0.43727402365717927\n",
            "Epoch loss: 0.4367103728282451\n",
            "Epoch loss: 0.43614331049892796\n",
            "Epoch loss: 0.4355718935750035\n",
            "Epoch loss: 0.434995359568292\n",
            "Epoch loss: 0.4344133318695229\n",
            "Epoch loss: 0.433826093366022\n",
            "Epoch loss: 0.43323484124610295\n",
            "Epoch loss: 0.43264173822503954\n",
            "Epoch loss: 0.43204957782962494\n",
            "Epoch loss: 0.4314611039154716\n",
            "Epoch loss: 0.43087833068362547\n",
            "Epoch loss: 0.4303022362578254\n",
            "Epoch loss: 0.4297328885558731\n",
            "Epoch loss: 0.4291697829355061\n",
            "Epoch loss: 0.4286121655748684\n",
            "Epoch loss: 0.4280592511576483\n",
            "Epoch loss: 0.4275103413180568\n",
            "Epoch loss: 0.42696487838557134\n",
            "Epoch loss: 0.4264224631542628\n",
            "Epoch loss: 0.4258828535348981\n",
            "Epoch loss: 0.4253459522608955\n",
            "Epoch loss: 0.42481178711182516\n",
            "Epoch loss: 0.4242804852826921\n",
            "Epoch loss: 0.4237522434557834\n",
            "Epoch loss: 0.4232272959017271\n",
            "Epoch loss: 0.4227058837329094\n",
            "Epoch loss: 0.422188228614495\n",
            "Epoch loss: 0.421674513518647\n",
            "Epoch loss: 0.4211648716613322\n",
            "Epoch loss: 0.4206593831146389\n",
            "Epoch loss: 0.4201580773186285\n",
            "Epoch loss: 0.41966093916003855\n",
            "Epoch loss: 0.41916791642546813\n",
            "Epoch loss: 0.4186789270124907\n",
            "Epoch loss: 0.4181938649778114\n",
            "Epoch loss: 0.4177126050916449\n",
            "Epoch loss: 0.41723500596023344\n",
            "Epoch loss: 0.416760911984716\n",
            "Epoch loss: 0.4162901545044484\n",
            "Epoch loss: 0.41582255249405464\n",
            "Epoch loss: 0.4153579132007078\n",
            "Epoch loss: 0.41489603315788043\n",
            "Epoch loss: 0.4144367001119349\n",
            "Epoch loss: 0.41397969654477934\n",
            "Epoch loss: 0.4135248056328299\n",
            "Epoch loss: 0.4130718205588176\n",
            "Epoch loss: 0.4126205579189922\n",
            "Epoch loss: 0.4121708752948997\n",
            "Epoch loss: 0.4117226916372575\n",
            "Epoch loss: 0.4112760069211069\n",
            "Epoch loss: 0.4108309151587078\n",
            "Epoch loss: 0.4103876037276628\n",
            "Epoch loss: 0.4099463339146212\n",
            "Epoch loss: 0.40950740332860514\n",
            "Epoch loss: 0.4090710984028965\n",
            "Epoch loss: 0.4086376501726995\n",
            "Epoch loss: 0.4082072053032594\n",
            "Epoch loss: 0.4077798175732552\n",
            "Epoch loss: 0.4073554571274998\n",
            "Epoch loss: 0.40693403014348134\n",
            "Epoch loss: 0.40651540126948627\n",
            "Epoch loss: 0.4060994136288937\n",
            "Epoch loss: 0.4056859040994471\n",
            "Epoch loss: 0.40527471366369494\n",
            "Epoch loss: 0.4048656936519927\n",
            "Epoch loss: 0.4044587089648226\n",
            "Epoch loss: 0.40405363924471194\n",
            "Epoch loss: 0.40365037873190346\n",
            "Epoch loss: 0.4032488353146103\n",
            "Epoch loss: 0.4028489291216035\n",
            "Epoch loss: 0.4024505909070124\n",
            "Epoch loss: 0.40205376043317975\n",
            "Epoch loss: 0.40165838505177687\n",
            "Epoch loss: 0.4012644187015695\n",
            "Epoch loss: 0.40087182157080564\n",
            "Epoch loss: 0.40048056070264926\n",
            "Epoch loss: 0.40009061184306494\n",
            "Epoch loss: 0.3997019628279721\n",
            "Epoch loss: 0.39931461875540597\n",
            "Epoch loss: 0.39892860904370897\n",
            "Epoch loss: 0.3985439961674509\n",
            "Epoch loss: 0.3981608853052998\n",
            "Epoch loss: 0.3977794332893637\n",
            "Epoch loss: 0.3973998542358404\n",
            "Epoch loss: 0.39702241848409364\n",
            "Epoch loss: 0.396647441709111\n",
            "Epoch loss: 0.39627526300989985\n",
            "Epoch loss: 0.3959062143781436\n",
            "Epoch loss: 0.3955405878388113\n",
            "Epoch loss: 0.39517860843158353\n",
            "Epoch loss: 0.39482041951730795\n",
            "Epoch loss: 0.3944660823656762\n",
            "Epoch loss: 0.3941155872179025\n",
            "Epoch loss: 0.3937688704322162\n",
            "Epoch loss: 0.3934258324762416\n",
            "Epoch loss: 0.3930863533502674\n",
            "Epoch loss: 0.3927503040784455\n",
            "Epoch loss: 0.39241755434961006\n",
            "Epoch loss: 0.39208797707814186\n",
            "Epoch loss: 0.39176145080305247\n",
            "Epoch loss: 0.3914378607190803\n",
            "Epoch loss: 0.3911170989249839\n",
            "Epoch loss: 0.3907990642763712\n",
            "Epoch loss: 0.39048366207668084\n",
            "Epoch loss: 0.3901708037330868\n",
            "Epoch loss: 0.3898604064355799\n",
            "Epoch loss: 0.38955239287674187\n",
            "Epoch loss: 0.389246691007652\n",
            "Epoch loss: 0.38894323381518414\n",
            "Epoch loss: 0.38864195910293986\n",
            "Epoch loss: 0.38834280925914544\n",
            "Epoch loss: 0.388045730998055\n",
            "Epoch loss: 0.3877506750655905\n",
            "Epoch loss: 0.38745759590437084\n",
            "Epoch loss: 0.3871664512775635\n",
            "Epoch loss: 0.38687720185482966\n",
            "Epoch loss: 0.3865898107668725\n",
            "Epoch loss: 0.38630424313760664\n",
            "Epoch loss: 0.38602046560464437\n",
            "Epoch loss: 0.38573844583961114\n",
            "Epoch loss: 0.3854581520797686\n",
            "Epoch loss: 0.38517955268161713\n",
            "Epoch loss: 0.38490261570572015\n",
            "Epoch loss: 0.38462730854013816\n",
            "Epoch loss: 0.3843535975678062\n",
            "Epoch loss: 0.3840814478811947\n",
            "Epoch loss: 0.38381082304589215\n",
            "Epoch loss: 0.3835416849135491\n",
            "Epoch loss: 0.3832739934841123\n",
            "Epoch loss: 0.38300770681757634\n",
            "Epoch loss: 0.38274278099670833\n",
            "Epoch loss: 0.3824791701443746\n",
            "Epoch loss: 0.38221682650227434\n",
            "Epoch loss: 0.38195570058195677\n",
            "Epoch loss: 0.38169574140382\n",
            "Epoch loss: 0.38143689684499466\n",
            "Epoch loss: 0.3811791141219038\n",
            "Epoch loss: 0.3809223404367577\n",
            "Epoch loss: 0.3806665238174641\n",
            "Epoch loss: 0.3804116141748578\n",
            "Epoch loss: 0.3801575645864626\n",
            "Epoch loss: 0.37990433278847413\n",
            "Epoch loss: 0.3796518828143599\n",
            "Epoch loss: 0.37940018665901465\n",
            "Epoch loss: 0.3791492257770235\n",
            "Epoch loss: 0.3788989921559846\n",
            "Epoch loss: 0.3786494886648052\n",
            "Epoch loss: 0.3784007283931063\n",
            "Epoch loss: 0.37815273279888245\n",
            "Epoch loss: 0.37790552867489785\n",
            "Epoch loss: 0.37765914420092456\n",
            "Epoch loss: 0.3774136045990779\n",
            "Epoch loss: 0.3771689280623246\n",
            "Epoch loss: 0.376925122610739\n",
            "Epoch loss: 0.37668218433624623\n",
            "Epoch loss: 0.3764400971878304\n",
            "Epoch loss: 0.3761988341334763\n",
            "Epoch loss: 0.37595835931177446\n",
            "Epoch loss: 0.3757186307030585\n",
            "Epoch loss: 0.3754796028945632\n",
            "Epoch loss: 0.3752412296355154\n",
            "Epoch loss: 0.3750034660190897\n",
            "Epoch loss: 0.37476627024779136\n",
            "Epoch loss: 0.37452960501881444\n",
            "Epoch loss: 0.3742934386057697\n",
            "Epoch loss: 0.37405774572154743\n",
            "Epoch loss: 0.3738225082346663\n",
            "Epoch loss: 0.3735877157873396\n",
            "Epoch loss: 0.37335336633406285\n",
            "Epoch loss: 0.37311946658920453\n",
            "Epoch loss: 0.37288603234425494\n",
            "Epoch loss: 0.37265308859342255\n",
            "Epoch loss: 0.372420669393762\n",
            "Epoch loss: 0.37218881738658205\n",
            "Epoch loss: 0.37195758292314696\n",
            "Epoch loss: 0.3717270227701398\n",
            "Epoch loss: 0.37149719841609014\n",
            "Epoch loss: 0.371268174052365\n",
            "Epoch loss: 0.3710400143517274\n",
            "Epoch loss: 0.37081278220333624\n",
            "Epoch loss: 0.37058653657677687\n",
            "Epoch loss: 0.3703613306753597\n",
            "Epoch loss: 0.3701372105026382\n",
            "Epoch loss: 0.3699142139135717\n",
            "Epoch loss: 0.3696923701639158\n",
            "Epoch loss: 0.36947169991914547\n",
            "Epoch loss: 0.3692522156456492\n",
            "Epoch loss: 0.3690339222857644\n",
            "Epoch loss: 0.36881681811394773\n",
            "Epoch loss: 0.3686008956803063\n",
            "Epoch loss: 0.3683861427648637\n",
            "Epoch loss: 0.36817254328638904\n",
            "Epoch loss: 0.367960078129591\n",
            "Epoch loss: 0.367748725871597\n",
            "Epoch loss: 0.3675384634018657\n",
            "Epoch loss: 0.36732926643894076\n",
            "Epoch loss: 0.3671211099532702\n",
            "Epoch loss: 0.3669139685084389\n",
            "Epoch loss: 0.3667078165343823\n",
            "Epoch loss: 0.36650262854614823\n",
            "Epoch loss: 0.36629837932108145\n",
            "Epoch loss: 0.36609504404631577\n",
            "Epoch loss: 0.36589259844739624\n",
            "Epoch loss: 0.3656910189078909\n",
            "Epoch loss: 0.3654902825890436\n",
            "Epoch loss: 0.3652903675578667\n",
            "Epoch loss: 0.3650912529315745\n",
            "Epoch loss: 0.36489291904580723\n",
            "Epoch loss: 0.364695347653637\n",
            "Epoch loss: 0.3644985221617196\n",
            "Epoch loss: 0.3643024279090381\n",
            "Epoch loss: 0.36410705249225833\n",
            "Epoch loss: 0.36391238613961385\n",
            "Epoch loss: 0.36371842213222855\n",
            "Epoch loss: 0.3635251572676804\n",
            "Epoch loss: 0.3633325923552841\n",
            "Epoch loss: 0.363140732726016\n",
            "Epoch loss: 0.3629495887324002\n",
            "Epoch loss: 0.3627591762054928\n",
            "Epoch loss: 0.36256951682817096\n",
            "Epoch loss: 0.3623806383774664\n",
            "Epoch loss: 0.36219257478523675\n",
            "Epoch loss: 0.36200536596783517\n",
            "Epoch loss: 0.36181905738319103\n",
            "Epoch loss: 0.3616336992889701\n",
            "Epoch loss: 0.36144934569815285\n",
            "Epoch loss: 0.3612660530569467\n",
            "Epoch loss: 0.36108387870113545\n",
            "Epoch loss: 0.36090287917615427\n",
            "Epoch loss: 0.36072310852806927\n",
            "Epoch loss: 0.36054461668261545\n",
            "Epoch loss: 0.36036744802470344\n",
            "Epoch loss: 0.36019164027142275\n",
            "Epoch loss: 0.36001722370053957\n",
            "Epoch loss: 0.359844220759193\n",
            "Epoch loss: 0.35967264604023\n",
            "Epoch loss: 0.35950250658212485\n",
            "Epoch loss: 0.3593338024266355\n",
            "Epoch loss: 0.35916652735776294\n",
            "Epoch loss: 0.3590006697454474\n",
            "Epoch loss: 0.3588362134254284\n",
            "Epoch loss: 0.3586731385597312\n",
            "Epoch loss: 0.3585114224373139\n",
            "Epoch loss: 0.35835104018913344\n",
            "Epoch loss: 0.358191965404664\n",
            "Epoch loss: 0.3580341706469398\n",
            "Epoch loss: 0.35787762787036675\n",
            "Epoch loss: 0.3577223087500908\n",
            "Epoch loss: 0.3575681849341253\n",
            "Epoch loss: 0.3574152282302687\n",
            "Epoch loss: 0.3572634107395658\n",
            "Epoch loss: 0.35711270494715636\n",
            "Epoch loss: 0.3569630837800626\n",
            "Epoch loss: 0.35681452064009106\n",
            "Epoch loss: 0.3566669894186441\n",
            "Epoch loss: 0.35652046449898245\n",
            "Epoch loss: 0.3563749207503669\n",
            "Epoch loss: 0.35623033351755323\n",
            "Epoch loss: 0.3560866786083251\n",
            "Epoch loss: 0.35594393228108884\n",
            "Epoch loss: 0.35580207123402885\n",
            "Epoch loss: 0.355661072596897\n",
            "Epoch loss: 0.35552091392616697\n",
            "Epoch loss: 0.35538157320402675\n",
            "Epoch loss: 0.3552430288414653\n",
            "Epoch loss: 0.3551052596855474\n",
            "Epoch loss: 0.3549682450308475\n",
            "Epoch loss: 0.3548319646348993\n",
            "Epoch loss: 0.3546963987374413\n",
            "Epoch loss: 0.3545615280831606\n",
            "Epoch loss: 0.3544273339475702\n",
            "Epoch loss: 0.3542937981655891\n",
            "Epoch loss: 0.3541609031623313\n",
            "Epoch loss: 0.354028631985534\n",
            "Epoch loss: 0.35389696833898976\n",
            "Epoch loss: 0.3537658966162616\n",
            "Epoch loss: 0.3536354019338915\n",
            "Epoch loss: 0.3535054701632269\n",
            "Epoch loss: 0.35337608795992687\n",
            "Epoch loss: 0.3532472427901506\n",
            "Epoch loss: 0.3531189229523911\n",
            "Epoch loss: 0.35299111759390833\n",
            "Epoch loss: 0.3528638167207429\n",
            "Epoch loss: 0.3527370112003584\n",
            "Epoch loss: 0.3526106927560736\n",
            "Epoch loss: 0.35248485395262447\n",
            "Epoch loss: 0.35235948817240903\n",
            "Epoch loss: 0.3522345895822457\n",
            "Epoch loss: 0.35211015309078847\n",
            "Epoch loss: 0.3519861742970766\n",
            "Epoch loss: 0.3518626494310523\n",
            "Epoch loss: 0.35173957528720756\n",
            "Epoch loss: 0.35161694915282404\n",
            "Epoch loss: 0.3514947687325028\n",
            "Epoch loss: 0.3513730320708375\n",
            "Epoch loss: 0.35125173747513366\n",
            "Epoch loss: 0.3511308834400259\n",
            "Epoch loss: 0.35101046857567847\n",
            "Epoch loss: 0.35089049154098384\n",
            "Epoch loss: 0.35077095098283223\n",
            "Epoch loss: 0.3506518454821022\n",
            "Epoch loss: 0.35053317350659724\n",
            "Epoch loss: 0.35041493337071244\n",
            "Epoch loss: 0.3502971232012259\n",
            "Epoch loss: 0.35017974090828824\n",
            "Epoch loss: 0.3500627841604601\n",
            "Epoch loss: 0.3499462503625367\n",
            "Epoch loss: 0.3498301366349231\n",
            "Epoch loss: 0.3497144397934617\n",
            "Epoch loss: 0.3495991563288854\n",
            "Epoch loss: 0.34948428238542595\n",
            "Epoch loss: 0.349369813738538\n",
            "Epoch loss: 0.34925574577217056\n",
            "Epoch loss: 0.34914207345647313\n",
            "Epoch loss: 0.34902879132723613\n",
            "Epoch loss: 0.34891589346868684\n",
            "Epoch loss: 0.34880337350145607\n",
            "Epoch loss: 0.3486912245775749\n",
            "Epoch loss: 0.3485794393842349\n",
            "Epoch loss: 0.3484680101577592\n",
            "Epoch loss: 0.3483569287087932\n",
            "Epoch loss: 0.3482461864591706\n",
            "Epoch loss: 0.3481357744902901\n",
            "Epoch loss: 0.3480256836021842\n",
            "Epoch loss: 0.3479159043818678\n",
            "Epoch loss: 0.34780642727902383\n",
            "Epoch loss: 0.3476972426867115\n",
            "Epoch loss: 0.347588341024564\n",
            "Epoch loss: 0.34747971282193274\n",
            "Epoch loss: 0.34737134879860065\n",
            "Epoch loss: 0.3472632399410488\n",
            "Epoch loss: 0.34715537757276693\n",
            "Epoch loss: 0.3470477534177124\n",
            "Epoch loss: 0.34694035965670034\n",
            "Epoch loss: 0.34683318897717286\n",
            "Epoch loss: 0.34672623461739926\n",
            "Epoch loss: 0.34661949040664936\n",
            "Epoch loss: 0.34651295080318156\n",
            "Epoch loss: 0.34640661093200614\n",
            "Epoch loss: 0.34630046662425756\n",
            "Epoch loss: 0.34619451445968125\n",
            "Epoch loss: 0.3460887518131999\n",
            "Epoch loss: 0.34598317690584585\n",
            "Epoch loss: 0.34587778885956005\n",
            "Epoch loss: 0.3457725877545526\n",
            "Epoch loss: 0.34566757468715115\n",
            "Epoch loss: 0.3455627518254156\n",
            "Epoch loss: 0.34545812245931445\n",
            "Epoch loss: 0.34535369104199387\n",
            "Epoch loss: 0.3452494632186544\n",
            "Epoch loss: 0.34514544583978235\n",
            "Epoch loss: 0.34504164695595557\n",
            "Epoch loss: 0.34493807579212965\n",
            "Epoch loss: 0.3448347427001609\n",
            "Epoch loss: 0.34473165908927206\n",
            "Epoch loss: 0.34462883733517174\n",
            "Epoch loss: 0.34452629066950935\n",
            "Epoch loss: 0.34442403305223174\n",
            "Epoch loss: 0.3443220790301544\n",
            "Epoch loss: 0.34422044358561604\n",
            "Epoch loss: 0.34411914197942933\n",
            "Epoch loss: 0.3440181895924572\n",
            "Epoch loss: 0.3439176017700431\n",
            "Epoch loss: 0.34381739367321884\n",
            "Epoch loss: 0.34371758014014026\n",
            "Epoch loss: 0.34361817556060603\n",
            "Epoch loss: 0.3435191937658242\n",
            "Epoch loss: 0.3434206479348888\n",
            "Epoch loss: 0.343322550518701\n",
            "Epoch loss: 0.343224913181422\n",
            "Epoch loss: 0.34312774675893964\n",
            "Epoch loss: 0.343031061233348\n",
            "Epoch loss: 0.34293486572203785\n",
            "Epoch loss: 0.34283916847972734\n",
            "Epoch loss: 0.34274397691158287\n",
            "Epoch loss: 0.3426492975955117\n",
            "Epoch loss: 0.34255513631171547\n",
            "Epoch loss: 0.34246149807767673\n",
            "Epoch loss: 0.3423683871868855\n",
            "Epoch loss: 0.3422758072497883\n",
            "Epoch loss: 0.34218376123563\n",
            "Epoch loss: 0.34209225151407574\n",
            "Epoch loss: 0.34200127989569074\n",
            "Epoch loss: 0.34191084767055835\n",
            "Epoch loss: 0.34182095564448306\n",
            "Epoch loss: 0.3417316041723845\n",
            "Epoch loss: 0.3416427931886159\n",
            "Epoch loss: 0.3415545222340497\n",
            "Epoch loss: 0.3414667904798578\n",
            "Epoch loss: 0.34137959674798835\n",
            "Epoch loss: 0.3412929395283924\n",
            "Epoch loss: 0.34120681699310773\n",
            "Epoch loss: 0.34112122700734576\n",
            "Epoch loss: 0.34103616713777596\n",
            "Epoch loss: 0.3409516346582415\n",
            "Epoch loss: 0.3408676265531886\n",
            "Epoch loss: 0.3407841395191389\n",
            "Epoch loss: 0.3407011699645775\n",
            "Epoch loss: 0.3406187140086731\n",
            "Epoch loss: 0.3405367674792747\n",
            "Epoch loss: 0.34045532591065125\n",
            "Epoch loss: 0.3403743845414395\n",
            "Epoch loss: 0.3402939383132451\n",
            "Epoch loss: 0.34021398187031315\n",
            "Epoch loss: 0.34013450956060887\n",
            "Epoch loss: 0.3400555154385914\n",
            "Epoch loss: 0.33997699326986014\n",
            "Epoch loss: 0.33989893653776504\n",
            "Epoch loss: 0.3398213384519636\n",
            "Epoch loss: 0.339744191958819\n",
            "Epoch loss: 0.3396674897534387\n",
            "Epoch loss: 0.3395912242930808\n",
            "Epoch loss: 0.3395153878115943\n",
            "Epoch loss: 0.33943997233451784\n",
            "Epoch loss: 0.33936496969443725\n",
            "Epoch loss: 0.33929037154619635\n",
            "Epoch loss: 0.3392161693815576\n",
            "Epoch loss: 0.33914235454293695\n",
            "Epoch loss: 0.3390689182358582\n",
            "Epoch loss: 0.33899585153981254\n",
            "Epoch loss: 0.33892314541724083\n",
            "Epoch loss: 0.33885079072040064\n",
            "Epoch loss: 0.3387787781959062\n",
            "Epoch loss: 0.33870709848676583\n",
            "Epoch loss: 0.33863574213176456\n",
            "Epoch loss: 0.33856469956205315\n",
            "Epoch loss: 0.3384939610948182\n",
            "Epoch loss: 0.33842351692390776\n",
            "Epoch loss: 0.3383533571072747\n",
            "Epoch loss: 0.33828347155109006\n",
            "Epoch loss: 0.33821384999034637\n",
            "Epoch loss: 0.338144481965737\n",
            "Epoch loss: 0.33807535679655254\n",
            "Epoch loss: 0.338006463549276\n",
            "Epoch loss: 0.3379377910014903\n",
            "Epoch loss: 0.33786932760062754\n",
            "Epoch loss: 0.33780106141699157\n",
            "Epoch loss: 0.3377329800903634\n",
            "Epoch loss: 0.3376650707693654\n",
            "Epoch loss: 0.337597320042581\n",
            "Epoch loss: 0.3375297138602344\n",
            "Epoch loss: 0.3374622374449855\n",
            "Epoch loss: 0.33739487519009825\n",
            "Epoch loss: 0.33732761054287946\n",
            "Epoch loss: 0.3372604258708451\n",
            "Epoch loss: 0.33719330230751227\n",
            "Epoch loss: 0.3371262195740401\n",
            "Epoch loss: 0.3370591557720815\n",
            "Epoch loss: 0.3369920871421285\n",
            "Epoch loss: 0.3369249877802672\n",
            "Epoch loss: 0.33685782930450253\n",
            "Epoch loss: 0.3367905804595639\n",
            "Epoch loss: 0.3367232066461814\n",
            "Epoch loss: 0.3366556693570096\n",
            "Epoch loss: 0.3365879254963662\n",
            "Epoch loss: 0.3365199265543166\n",
            "Epoch loss: 0.33645161759678116\n",
            "Epoch loss: 0.3363829360214597\n",
            "Epoch loss: 0.3363138100133057\n",
            "Epoch loss: 0.33624415661145785\n",
            "Epoch loss: 0.33617387926976344\n",
            "Epoch loss: 0.33610286475231654\n",
            "Epoch loss: 0.33603097914984553\n",
            "Epoch loss: 0.33595806272742124\n",
            "Epoch loss: 0.33588392321344734\n",
            "Epoch loss: 0.3358083270104572\n",
            "Epoch loss: 0.335730987653403\n",
            "Epoch loss: 0.33565155068664787\n",
            "Epoch loss: 0.3355695740614454\n",
            "Epoch loss: 0.3354845034012164\n",
            "Epoch loss: 0.3353956426132382\n",
            "Epoch loss: 0.33530212366926393\n",
            "Epoch loss: 0.33520288778824825\n",
            "Epoch loss: 0.33509670908666983\n",
            "Epoch loss: 0.3349823280734702\n",
            "Epoch loss: 0.3348588136779441\n",
            "Epoch loss: 0.3347262830291458\n",
            "Epoch loss: 0.3345868891184967\n",
            "Epoch loss: 0.33444531813458084\n",
            "Epoch loss: 0.3343075007684893\n",
            "Epoch loss: 0.3341776776905766\n",
            "Epoch loss: 0.33405662203379793\n",
            "Epoch loss: 0.3339426809536019\n",
            "Epoch loss: 0.33383369086023434\n",
            "Epoch loss: 0.33372792441896637\n",
            "Epoch loss: 0.3336241972497458\n",
            "Epoch loss: 0.3335217278963915\n",
            "Epoch loss: 0.3334200048667807\n",
            "Epoch loss: 0.3333186998448759\n",
            "Epoch loss: 0.3332176144050948\n",
            "Epoch loss: 0.3331166460483845\n",
            "Epoch loss: 0.33301576482735284\n",
            "Epoch loss: 0.3329149959047718\n",
            "Epoch loss: 0.3328144057364865\n",
            "Epoch loss: 0.3327140907805266\n",
            "Epoch loss: 0.3326141681714159\n",
            "Epoch loss: 0.3325147679740329\n",
            "Epoch loss: 0.3324160266620514\n",
            "Epoch loss: 0.3323180814906369\n",
            "Epoch loss: 0.33222106551334685\n",
            "Epoch loss: 0.3321251031257832\n",
            "Epoch loss: 0.33203030616203094\n",
            "Epoch loss: 0.33193677067613614\n",
            "Epoch loss: 0.3318445745788956\n",
            "Epoch loss: 0.33175377626615066\n",
            "Epoch loss: 0.3316644142881819\n",
            "Epoch loss: 0.3315765080024389\n",
            "Epoch loss: 0.33149005905489387\n",
            "Epoch loss: 0.33140505347031834\n",
            "Epoch loss: 0.33132146410708607\n",
            "Epoch loss: 0.3312392532443813\n",
            "Epoch loss: 0.3311583751082922\n",
            "Epoch loss: 0.33107877819522213\n",
            "Epoch loss: 0.33100040730479713\n",
            "Epoch loss: 0.33092320524189417\n",
            "Epoch loss: 0.33084711418433055\n",
            "Epoch loss: 0.33077207673807935\n",
            "Epoch loss: 0.3306980367167202\n",
            "Epoch loss: 0.3306249396883107\n",
            "Epoch loss: 0.3305527333334393\n",
            "Epoch loss: 0.3304813676550385\n",
            "Epoch loss: 0.3304107950753912\n",
            "Epoch loss: 0.3303409704498672\n",
            "Epoch loss: 0.33027185102110806\n",
            "Epoch loss: 0.33020339633208395\n",
            "Epoch loss: 0.33013556811190115\n",
            "Epoch loss: 0.3300683301444884\n",
            "Epoch loss: 0.3300016481273026\n",
            "Epoch loss: 0.3299354895248959\n",
            "Epoch loss: 0.3298698234204527\n",
            "Epoch loss: 0.32980462036717\n",
            "Epoch loss: 0.32973985224049235\n",
            "Epoch loss: 0.32967549209167124\n",
            "Epoch loss: 0.32961151400281247\n",
            "Epoch loss: 0.3295478929434441\n",
            "Epoch loss: 0.3294846046286466\n",
            "Epoch loss: 0.3294216253788575\n",
            "Epoch loss: 0.3293589319815931\n",
            "Epoch loss: 0.3292965015554499\n",
            "Epoch loss: 0.3292343114168447\n",
            "Epoch loss: 0.32917233895000747\n",
            "Epoch loss: 0.32911056148071793\n",
            "Epoch loss: 0.32904895615420365\n",
            "Epoch loss: 0.3289874998174737\n",
            "Epoch loss: 0.3289261689061794\n",
            "Epoch loss: 0.3288649393359025\n",
            "Epoch loss: 0.3288037863976032\n",
            "Epoch loss: 0.328742684656859\n",
            "Epoch loss: 0.32868160785655237\n",
            "Epoch loss: 0.3286205288228517\n",
            "Epoch loss: 0.3285594193747467\n",
            "Epoch loss: 0.3284982502380711\n",
            "Epoch loss: 0.3284369909659175\n",
            "Epoch loss: 0.3283756098686301\n",
            "Epoch loss: 0.32831407395815004\n",
            "Epoch loss: 0.3282523489133299\n",
            "Epoch loss: 0.32819039907486747\n",
            "Epoch loss: 0.3281281874805931\n",
            "Epoch loss: 0.3280656759538305\n",
            "Epoch loss: 0.32800282525924873\n",
            "Epoch loss: 0.32793959534185496\n",
            "Epoch loss: 0.32787594566545025\n",
            "Epoch loss: 0.3278118356669856\n",
            "Epoch loss: 0.32774722534305545\n",
            "Epoch loss: 0.3276820759846889\n",
            "Epoch loss: 0.32761635107727266\n",
            "Epoch loss: 0.32755001738440287\n",
            "Epoch loss: 0.3274830462377357\n",
            "Epoch loss: 0.32741541505824684\n",
            "Epoch loss: 0.3273471091343986\n",
            "Epoch loss: 0.3272781236735821\n",
            "Epoch loss: 0.3272084661164426\n",
            "Epoch loss: 0.32713815865029483\n",
            "Epoch loss: 0.32706724077221433\n",
            "Epoch loss: 0.326995771638369\n",
            "Epoch loss: 0.32692383181352674\n",
            "Epoch loss: 0.3268515239422784\n",
            "Epoch loss: 0.3267789718541431\n",
            "Epoch loss: 0.3267063177373609\n",
            "Epoch loss: 0.3266337172880909\n",
            "Epoch loss: 0.3265613331222516\n",
            "Epoch loss: 0.3264893271238665\n",
            "Epoch loss: 0.3264178526649162\n",
            "Epoch loss: 0.32634704766799627\n",
            "Epoch loss: 0.3262770292816181\n",
            "Epoch loss: 0.3262078905805768\n",
            "Epoch loss: 0.32613969932078396\n",
            "Epoch loss: 0.32607249848061626\n",
            "Epoch loss: 0.3260063081583729\n",
            "Epoch loss: 0.3259411283562879\n",
            "Epoch loss: 0.32587694222365016\n",
            "Epoch loss: 0.3258137194127297\n",
            "Epoch loss: 0.3257514192931867\n",
            "Epoch loss: 0.3256899938587414\n",
            "Epoch loss: 0.32562939023715487\n",
            "Epoch loss: 0.3255695527775126\n",
            "Epoch loss: 0.325510424735834\n",
            "Epoch loss: 0.3254519496113364\n",
            "Epoch loss: 0.32539407220305594\n",
            "Epoch loss: 0.32533673946269426\n",
            "Epoch loss: 0.3252799012174033\n",
            "Epoch loss: 0.3252235108280334\n",
            "Epoch loss: 0.32516752583558267\n",
            "Epoch loss: 0.3251119086316836\n",
            "Epoch loss: 0.32505662716784395\n",
            "Epoch loss: 0.32500165569266665\n",
            "Epoch loss: 0.3249469754769621\n",
            "Epoch loss: 0.3248925754556766\n",
            "Epoch loss: 0.32483845268735867\n",
            "Epoch loss: 0.32478461251348945\n",
            "Epoch loss: 0.3247310683000387\n",
            "Epoch loss: 0.32467784066995486\n",
            "Epoch loss: 0.3246249561913431\n",
            "Epoch loss: 0.3245724455668774\n",
            "Epoch loss: 0.3245203414604911\n",
            "Epoch loss: 0.32446867617480984\n",
            "Epoch loss: 0.3244174794336917\n",
            "Epoch loss: 0.32436677651383566\n",
            "Epoch loss: 0.32431658690854115\n",
            "Epoch loss: 0.32426692361230386\n",
            "Epoch loss: 0.3242177930133371\n",
            "Epoch loss: 0.3241691952976833\n",
            "Epoch loss: 0.32412112521905734\n",
            "Epoch loss: 0.3240735730758555\n",
            "Epoch loss: 0.32402652575344676\n",
            "Epoch loss: 0.32397996772353144\n",
            "Epoch loss: 0.32393388193092787\n",
            "Epoch loss: 0.3238882505329824\n",
            "Epoch loss: 0.3238430554834678\n",
            "Epoch loss: 0.32379827897039876\n",
            "Epoch loss: 0.32375390372678964\n",
            "Epoch loss: 0.32370991323705767\n",
            "Epoch loss: 0.32366629186160245\n",
            "Epoch loss: 0.32362302489978345\n",
            "Epoch loss: 0.32358009860828824\n",
            "Epoch loss: 0.32353750018850214\n",
            "Epoch loss: 0.32349521775337853\n",
            "Epoch loss: 0.3234532402816487\n",
            "Epoch loss: 0.3234115575650652\n",
            "Epoch loss: 0.3233701601526857\n",
            "Epoch loss: 0.3233290392949328\n",
            "Epoch loss: 0.32328818688922245\n",
            "Epoch loss: 0.32324759542827386\n",
            "Epoch loss: 0.3232072579517237\n",
            "Epoch loss: 0.3231671680013428\n",
            "Epoch loss: 0.3231273195799179\n",
            "Epoch loss: 0.323087707113721\n",
            "Epoch loss: 0.3230483254183934\n",
            "Epoch loss: 0.3230091696680207\n",
            "Epoch loss: 0.322970235367149\n",
            "Epoch loss: 0.3229315183254806\n",
            "Epoch loss: 0.32289301463500075\n",
            "Epoch loss: 0.32285472064928267\n",
            "Epoch loss: 0.3228166329647479\n",
            "Epoch loss: 0.322778748403665\n",
            "Epoch loss: 0.3227410639986909\n",
            "Epoch loss: 0.32270357697877616\n",
            "Epoch loss: 0.3226662847562718\n",
            "Epoch loss: 0.32262918491508763\n",
            "Epoch loss: 0.32259227519976863\n",
            "Epoch loss: 0.32255555350536647\n",
            "Epoch loss: 0.32251901786799436\n",
            "Epoch loss: 0.32248266645596324\n",
            "Epoch loss: 0.3224464975614058\n",
            "Epoch loss: 0.32241050959230194\n",
            "Epoch loss: 0.32237470106482663\n",
            "Epoch loss: 0.3223390705959486\n",
            "Epoch loss: 0.322303616896211\n",
            "Epoch loss: 0.3222683387626353\n",
            "Epoch loss: 0.3222332350716923\n",
            "Epoch loss: 0.3221983047722927\n",
            "Epoch loss: 0.32216354687875415\n",
            "Epoch loss: 0.322128960463711\n",
            "Epoch loss: 0.32209454465094034\n",
            "Epoch loss: 0.32206029860808466\n",
            "Epoch loss: 0.32202622153926386\n",
            "Epoch loss: 0.32199231267757566\n",
            "Epoch loss: 0.32195857127749783\n",
            "Epoch loss: 0.32192499660721163\n",
            "Epoch loss: 0.3218915879408808\n",
            "Epoch loss: 0.32185834455092804\n",
            "Epoch loss: 0.3218252657003615\n",
            "Epoch loss: 0.3217923506352133\n",
            "Epoch loss: 0.3217595985771574\n",
            "Epoch loss: 0.3217270087163828\n",
            "Epoch loss: 0.321694580204799\n",
            "Epoch loss: 0.32166231214965324\n",
            "Epoch loss: 0.3216302036076379\n",
            "Epoch loss: 0.32159825357956334\n",
            "Epoch loss: 0.32156646100566383\n",
            "Epoch loss: 0.3215348247615968\n",
            "Epoch loss: 0.32150334365518757\n",
            "Epoch loss: 0.32147201642395373\n",
            "Epoch loss: 0.32144084173343673\n",
            "Epoch loss: 0.321409818176349\n",
            "Epoch loss: 0.32137894427253466\n",
            "Epoch loss: 0.32134821846972206\n",
            "Epoch loss: 0.32131763914504125\n",
            "Epoch loss: 0.3212872046072593\n",
            "Epoch loss: 0.32125691309968063\n",
            "Epoch loss: 0.32122676280364987\n",
            "Epoch loss: 0.32119675184258706\n",
            "Epoch loss: 0.3211668782864827\n",
            "Epoch loss: 0.3211371401567748\n",
            "Epoch loss: 0.32110753543153314\n",
            "Epoch loss: 0.3210780620508753\n",
            "Epoch loss: 0.3210487179225433\n",
            "Epoch loss: 0.3210195009275739\n",
            "Epoch loss: 0.3209904089260025\n",
            "Epoch loss: 0.32096143976254543\n",
            "Epoch loss: 0.32093259127221474\n",
            "Epoch loss: 0.32090386128582465\n",
            "Epoch loss: 0.3208752476353594\n",
            "Epoch loss: 0.3208467481591758\n",
            "Epoch loss: 0.3208183607070233\n",
            "Epoch loss: 0.3207900831448701\n",
            "Epoch loss: 0.3207619133595278\n",
            "Epoch loss: 0.32073384926307597\n",
            "Epoch loss: 0.320705888797086\n",
            "Epoch loss: 0.32067802993665395\n",
            "Epoch loss: 0.32065027069424884\n",
            "Epoch loss: 0.32062260912338814\n",
            "Epoch loss: 0.320595043322152\n",
            "Epoch loss: 0.32056757143654735\n",
            "Epoch loss: 0.32054019166373576\n",
            "Epoch loss: 0.3205129022551333\n",
            "Epoch loss: 0.3204857015193938\n",
            "Epoch loss: 0.3204585878252822\n",
            "Epoch loss: 0.32043155960444347\n",
            "Epoch loss: 0.32040461535406967\n",
            "Epoch loss: 0.32037775363946497\n",
            "Epoch loss: 0.3203509730965027\n",
            "Epoch loss: 0.32032427243397216\n",
            "Epoch loss: 0.3202976504357979\n",
            "Epoch loss: 0.3202711059631225\n",
            "Epoch loss: 0.32024463795623276\n",
            "Epoch loss: 0.3202182454363062\n",
            "Epoch loss: 0.3201919275069573\n",
            "Epoch loss: 0.3201656833555523\n",
            "Epoch loss: 0.3201395122542641\n",
            "Epoch loss: 0.32011341356083584\n",
            "Epoch loss: 0.32008738671901865\n",
            "Epoch loss: 0.32006143125865166\n",
            "Epoch loss: 0.3200355467953491\n",
            "Epoch loss: 0.3200097330297648\n",
            "Epoch loss: 0.3199839897464009\n",
            "Epoch loss: 0.31995831681193876\n",
            "Epoch loss: 0.3199327141730662\n",
            "Epoch loss: 0.3199071818537869\n",
            "Epoch loss: 0.31988171995220127\n",
            "Epoch loss: 0.31985632863675406\n",
            "Epoch loss: 0.319831008141958\n",
            "Epoch loss: 0.31980575876360295\n",
            "Epoch loss: 0.31978058085347627\n",
            "Epoch loss: 0.3197554748136252\n",
            "Epoch loss: 0.31973044109020277\n",
            "Epoch loss: 0.31970548016694667\n",
            "Epoch loss: 0.31968059255834846\n",
            "Epoch loss: 0.31965577880257867\n",
            "Epoch loss: 0.3196310394542363\n",
            "Epoch loss: 0.31960637507699996\n",
            "Epoch loss: 0.3195817862362561\n",
            "Epoch loss: 0.3195572734917848\n",
            "Epoch loss: 0.3195328373905808\n",
            "Epoch loss: 0.3195084784598845\n",
            "Epoch loss: 0.3194841972004979\n",
            "Epoch loss: 0.31945999408044895\n",
            "Epoch loss: 0.3194358695290649\n",
            "Epoch loss: 0.3194118239315075\n",
            "Epoch loss: 0.31938785762381083\n",
            "Epoch loss: 0.3193639708884552\n",
            "Epoch loss: 0.31934016395050135\n",
            "Epoch loss: 0.3193164369742976\n",
            "Epoch loss: 0.3192927900607627\n",
            "Epoch loss: 0.31926922324524015\n",
            "Epoch loss: 0.31924573649590904\n",
            "Epoch loss: 0.3192223297127294\n",
            "Epoch loss: 0.3191990027268938\n",
            "Epoch loss: 0.3191757553007503\n",
            "Epoch loss: 0.3191525871281597\n",
            "Epoch loss: 0.3191294978352415\n",
            "Epoch loss: 0.3191064869814677\n",
            "Epoch loss: 0.31908355406105604\n",
            "Epoch loss: 0.3190606985046178\n",
            "Epoch loss: 0.31903791968101314\n",
            "Epoch loss: 0.31901521689937057\n",
            "Epoch loss: 0.3189925894112276\n",
            "Epoch loss: 0.31897003641275057\n",
            "Epoch loss: 0.31894755704699845\n",
            "Epoch loss: 0.31892515040619296\n",
            "Epoch loss: 0.31890281553396443\n",
            "Epoch loss: 0.3188805514275441\n",
            "Epoch loss: 0.3188583570398777\n",
            "Epoch loss: 0.3188362312816354\n",
            "Epoch loss: 0.3188141730231006\n",
            "Epoch loss: 0.3187921810959177\n",
            "Epoch loss: 0.3187702542946852\n",
            "Epoch loss: 0.3187483913783794\n",
            "Epoch loss: 0.3187265910715997\n",
            "Epoch loss: 0.31870485206562277\n",
            "Epoch loss: 0.3186831730192602\n",
            "Epoch loss: 0.31866155255950934\n",
            "Epoch loss: 0.3186399892819926\n",
            "Epoch loss: 0.318618481751177\n",
            "Epoch loss: 0.3185970285003701\n",
            "Epoch loss: 0.3185756280314836\n",
            "Epoch loss: 0.3185542788145591\n",
            "Epoch loss: 0.3185329792870495\n",
            "Epoch loss: 0.31851172785284637\n",
            "Epoch loss: 0.31849052288104585\n",
            "Epoch loss: 0.31846936270444137\n",
            "Epoch loss: 0.3184482456177322\n",
            "Epoch loss: 0.31842716987543335\n",
            "Epoch loss: 0.318406133689471\n",
            "Epoch loss: 0.3183851352264458\n",
            "Epoch loss: 0.31836417260454064\n",
            "Epoch loss: 0.3183432438900528\n",
            "Epoch loss: 0.31832234709351637\n",
            "Epoch loss: 0.31830148016538923\n",
            "Epoch loss: 0.31828064099126335\n",
            "Epoch loss: 0.31825982738655845\n",
            "Epoch loss: 0.31823903709065\n",
            "Epoch loss: 0.31821826776037765\n",
            "Epoch loss: 0.3181975169628702\n",
            "Epoch loss: 0.31817678216761464\n",
            "Epoch loss: 0.3181560607376896\n",
            "Epoch loss: 0.31813534992006387\n",
            "Epoch loss: 0.31811464683485696\n",
            "Epoch loss: 0.3180939484634323\n",
            "Epoch loss: 0.3180732516351823\n",
            "Epoch loss: 0.3180525530128395\n",
            "Epoch loss: 0.3180318490761209\n",
            "Epoch loss: 0.3180111361034869\n",
            "Epoch loss: 0.31799041015175633\n",
            "Epoch loss: 0.31796966703328156\n",
            "Epoch loss: 0.3179489022903354\n",
            "Epoch loss: 0.3179281111663077\n",
            "Epoch loss: 0.3179072885732365\n",
            "Epoch loss: 0.3178864290551238\n",
            "Epoch loss: 0.31786552674638374\n",
            "Epoch loss: 0.3178445753246619\n",
            "Epoch loss: 0.3178235679571251\n",
            "Epoch loss: 0.3178024972391602\n",
            "Epoch loss: 0.31778135512422834\n",
            "Epoch loss: 0.3177601328433874\n",
            "Epoch loss: 0.3177388208127205\n",
            "Epoch loss: 0.3177174085265837\n",
            "Epoch loss: 0.3176958844341887\n",
            "Epoch loss: 0.3176742357965768\n",
            "Epoch loss: 0.31765244852048913\n",
            "Epoch loss: 0.3176305069649956\n",
            "Epoch loss: 0.3176083937159908\n",
            "Epoch loss: 0.31758608932280735\n",
            "Epoch loss: 0.3175635719902191\n",
            "Epoch loss: 0.3175408172180495\n",
            "Epoch loss: 0.31751779737947855\n",
            "Epoch loss: 0.31749448122807844\n",
            "Epoch loss: 0.31747083332273673\n",
            "Epoch loss: 0.31744681335925545\n",
            "Epoch loss: 0.3174223753980128\n",
            "Epoch loss: 0.31739746697943694\n",
            "Epoch loss: 0.3173720281244365\n",
            "Epoch loss: 0.31734599022738114\n",
            "Epoch loss: 0.31731927486788436\n",
            "Epoch loss: 0.3172917925993751\n",
            "Epoch loss: 0.3172634418246064\n",
            "Epoch loss: 0.31723410795161017\n",
            "Epoch loss: 0.3172036631532771\n",
            "Epoch loss: 0.31717196724917934\n",
            "Epoch loss: 0.31713887051040457\n"
          ]
        }
      ],
      "source": [
        "mean_epoch_error_list = list()\n",
        "for e in range(num_epochs):\n",
        "    epoch_loss_list = list()\n",
        "    for i in range(num_data_samples):\n",
        "        inp = np.expand_dims(inp_data[i], 1)\n",
        "        out_true = np.expand_dims(out_true_data[i], 1)\n",
        "        #print(inp.shape)\n",
        "        compute_grads(inp, out_true)\n",
        "        update_param()\n",
        "        out = predict(inp)\n",
        "        error = np.mean(mse(out, out_true))\n",
        "        epoch_loss_list.append(error)\n",
        "    mean_epoch_error = np.mean(np.array(epoch_loss_list))\n",
        "    mean_epoch_error_list.append(mean_epoch_error)\n",
        "    print(\"Epoch loss: {}\".format(mean_epoch_error))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "X5ISJZNMr2sr",
        "outputId": "b5d5f054-4669-4107-c7e4-8e3972df707e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x12c0b9cd0>]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "## Plot the model performance\n",
        "plt.plot(np.arange(0, num_epochs, 1), mean_epoch_error_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### load mnist data using\n",
        "#### mnist = tf.keras.datasets.mnist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define the following hyper parameters\n",
        "#### Parameters to make a new neural network with the correct input and output sizes\n",
        "#### Learning rate\n",
        "#### number of layers\n",
        "#### number of epochs\n",
        "#### Refer to cell (8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Reuse the layer dictionaries, weight dictionary and bias dictionary in cells (8) and (9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### If necessary, write a function to onehot encode the scalar labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define a function to calculate the accuracy (Use a threshold value to determine the sigmoid firing at the output layer when calculating the accuracy (This is because of the sigmoid function, If we use softmax, the logits are normalised so taking the maximum of the activation gives the predicted class))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train the neural network and plot the following (Extend the cell (18) accordingly)\n",
        "#### For validation directly use the test set\n",
        "#### Mean train error per epoch (Plot for all the epochs)\n",
        "#### Mean validation error per epoch (Plot for all the epochs)\n",
        "#### Mean train accuracy per epoch (Plot for all the epochs)\n",
        "#### Mean validation accuracy per epoch (Plot for all the epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KAxjdmBVr5PB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "ANN_Numpy_Regression.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "cc689c0049f72604b61cc19453d21a95ef7f4b2054b7d53ec7a9c4b537ba7e7d"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
